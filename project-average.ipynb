{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ce7519-7135-41c7-b36e-34cd83a5eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# For word embeddings\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a43a9b-49f0-4146-84b5-41c162aee6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all text files from a given directory and returns a list of their contents.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):  # Only process .txt files\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf8') as f:\n",
    "                    reviews.append(f.read().strip())\n",
    "    return reviews\n",
    "    \n",
    "# Directory paths (adjust these paths as needed)\n",
    "train_dir = 'train'  # Assumes the 'train' directory is in the current working directory\n",
    "test_dir = 'test'    # Assumes the 'test' directory is in the current working directory\n",
    "\n",
    "# Load negative and positive reviews from training data\n",
    "train_reviews_neg = load_reviews_from_directory(os.path.join(train_dir, 'neg'))\n",
    "train_reviews_pos = load_reviews_from_directory(os.path.join(train_dir, 'pos'))\n",
    "\n",
    "# Combine negative and positive training reviews and labels\n",
    "reviews_train = train_reviews_neg + train_reviews_pos\n",
    "train_labels = [0]*len(train_reviews_neg) + [1]*len(train_reviews_pos)  # 0: negative, 1: positive\n",
    "\n",
    "# Load negative and positive reviews from testing data\n",
    "test_reviews_neg = load_reviews_from_directory(os.path.join(test_dir, 'neg'))\n",
    "test_reviews_pos = load_reviews_from_directory(os.path.join(test_dir, 'pos'))\n",
    "\n",
    "# Combine negative and positive testing reviews and labels\n",
    "reviews_test = test_reviews_neg + test_reviews_pos\n",
    "test_labels = [0]*len(test_reviews_neg) + [1]*len(test_reviews_pos)  # 0: negative, 1: positive\n",
    "\n",
    "# Regular expressions for text cleaning\n",
    "REPLACE_NO_SPACE = re.compile(r\"[\\.;:!\\?',\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(r\"(<br\\s*/><br\\s*/>)|[-/]\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "    - Converting to lowercase\n",
    "    - Removing certain punctuation marks\n",
    "    - Replacing some patterns with space\n",
    "    \"\"\"\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "# Clean training and testing data\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)\n",
    "\n",
    "# Import gensim library\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300-SLIM.bin', binary=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c75097-7348-42f4-a029-7d19bb6b0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(embed_lookup, reviews):\n",
    "    \"\"\"\n",
    "    Tokenizes the reviews using the pre-trained Word2Vec model's vocabulary.\n",
    "    Words not in the vocabulary are mapped to index 0.\n",
    "    \"\"\"\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews:\n",
    "        tokens = []\n",
    "        for word in review.split():\n",
    "            if word in embed_lookup.key_to_index:\n",
    "                tokens.append(embed_lookup.key_to_index[word])\n",
    "            else:\n",
    "                tokens.append(0)  # Unknown words mapped to 0\n",
    "        tokenized_reviews.append(tokens)\n",
    "    return tokenized_reviews\n",
    "\n",
    "# Tokenize training and testing reviews\n",
    "tokenized_reviews_train = tokenize_reviews(embed_lookup, reviews_train_clean)\n",
    "tokenized_reviews_test = tokenize_reviews(embed_lookup, reviews_test_clean)\n",
    "\n",
    "# Remove zero-length reviews from training data\n",
    "non_zero_idx_train = [idx for idx, review in enumerate(tokenized_reviews_train) if len(review) != 0]\n",
    "tokenized_reviews_train = [tokenized_reviews_train[idx] for idx in non_zero_idx_train]\n",
    "train_labels = [train_labels[idx] for idx in non_zero_idx_train]\n",
    "\n",
    "# Remove zero-length reviews from testing data\n",
    "non_zero_idx_test = [idx for idx, review in enumerate(tokenized_reviews_test) if len(review) != 0]\n",
    "tokenized_reviews_test = [tokenized_reviews_test[idx] for idx in non_zero_idx_test]\n",
    "test_labels = [test_labels[idx] for idx in non_zero_idx_test]\n",
    "\n",
    "def pad_features(tokenized_reviews, seq_length):\n",
    "    \"\"\"\n",
    "    Return features of tokenized_reviews, where each review is padded with 0's \n",
    "    or truncated to the input seq_length.\n",
    "    \"\"\"\n",
    "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)\n",
    "    \n",
    "    for i, review in enumerate(tokenized_reviews):\n",
    "        if len(review) <= seq_length:\n",
    "            features[i, -len(review):] = np.array(review)\n",
    "        else:\n",
    "            features[i, :] = np.array(review[:seq_length])\n",
    "    return features\n",
    "\n",
    "# Set sequence length (e.g., 200)\n",
    "seq_length = 200\n",
    "\n",
    "# Pad training and testing data\n",
    "features_train = pad_features(tokenized_reviews_train, seq_length)\n",
    "features_test = pad_features(tokenized_reviews_test, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21316cc7-68c1-48b6-b5d4-6754d33b3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(17500, 200) \n",
      "Validation set: \t(7500, 200) \n",
      "Test set: \t\t(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Randomly split training data into training and validation sets\n",
    "train_x, val_x, train_y, val_y = train_test_split(\n",
    "    features_train, train_labels, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(features_test.shape))\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(test_labels))\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e203d5-3201-452a-b9a7-a4e986932bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 25, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 25, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 25, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=75, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, output_size, num_filters=100, kernel_sizes=[3, 4, 5], \n",
    "                 freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # 1. Embedding layer\n",
    "        # Get embeddings from the pre-trained model\n",
    "        embedding_dim = embed_model.vector_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embed_model.vectors), freeze=freeze_embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. Convolutional layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embedding_dim), padding=(k-2, 0)) \n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # 3. Fully-connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "\n",
    "        # 4. Dropout and activation\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional layer with ReLU activation and average pooling.\n",
    "        \"\"\"\n",
    "        x = F.relu(conv(x)).squeeze(3)  # Apply convolution and ReLU, then remove the last dimension\n",
    "        x = torch.mean(x, dim=2)  # Calculate the mean along the sequence length dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how the model processes the input data.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # Embedding layer\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for convolutional layer\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        x = [self.conv_and_pool(x, conv) for conv in self.convs_1d]\n",
    "\n",
    "        # Concatenate outputs and apply dropout\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully-connected layer and sigmoid activation\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "output_size = 1  # Binary classification\n",
    "num_filters = 25\n",
    "kernel_sizes = [3, 4, 5]\n",
    "dropout_prob = 0.5\n",
    "freeze_embeddings = True  # Set to False to fine-tune embeddings\n",
    "\n",
    "net = SentimentCNN(\n",
    "    embed_model=embed_lookup, \n",
    "    output_size=output_size, \n",
    "    num_filters=num_filters, \n",
    "    kernel_sizes=kernel_sizes, \n",
    "    freeze_embeddings=freeze_embeddings, \n",
    "    drop_prob=dropout_prob\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a421a13-8c51-4299-889e-3c6401e39ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Step: 100, Loss: 0.678006, Val Loss: 0.675104\n",
      "Epoch: 1/2, Step: 200, Loss: 0.614469, Val Loss: 0.647637\n",
      "Epoch: 1/2, Step: 300, Loss: 0.579992, Val Loss: 0.609817\n",
      "Epoch: 2/2, Step: 400, Loss: 0.564265, Val Loss: 0.577117\n",
      "Epoch: 2/2, Step: 500, Loss: 0.520638, Val Loss: 0.546264\n",
      "Epoch: 2/2, Step: 600, Loss: 0.526612, Val Loss: 0.522838\n",
      "Epoch: 2/2, Step: 700, Loss: 0.515532, Val Loss: 0.512718\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "def train(net, train_loader, valid_loader, epochs, print_every=100):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    net.train()\n",
    "    counter = 0  # For printing\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            # Zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # Get output from the model\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss statistics\n",
    "            if counter % print_every == 0:\n",
    "                net.eval()\n",
    "                val_losses = []\n",
    "                for val_inputs, val_labels in valid_loader:\n",
    "                    val_outputs = net(val_inputs)\n",
    "                    val_loss = criterion(val_outputs.squeeze(), val_labels.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                net.train()\n",
    "                print(f\"Epoch: {e+1}/{epochs}, Step: {counter}, \"\n",
    "                      f\"Loss: {loss.item():.6f}, Val Loss: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 2  # Adjust as needed\n",
    "print_every = 100\n",
    "\n",
    "# Train the model\n",
    "train(net, train_loader, valid_loader, epochs, print_every)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6802585d-79fc-43ad-bd27-baf15a40a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "Accuracy: 0.7759\n",
      "Precision: 0.8784\n",
      "Recall: 0.6401\n",
      "F1 Score: 0.7405\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7171    0.9115    0.8027      3752\n",
      "           1     0.8784    0.6401    0.7405      3748\n",
      "\n",
      "    accuracy                         0.7759      7500\n",
      "   macro avg     0.7978    0.7758    0.7716      7500\n",
      "weighted avg     0.7977    0.7759    0.7717      7500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3420  332]\n",
      " [1349 2399]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_on_validation(net, valid_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the validation dataset and print out accuracy metrics.\n",
    "    \"\"\"\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = net(inputs)\n",
    "            # Apply sigmoid and round to get binary predictions\n",
    "            preds_batch = torch.round(outputs.squeeze())\n",
    "            preds_batch = preds_batch.numpy()\n",
    "            labels_batch = labels.numpy()\n",
    "            preds.extend(preds_batch)\n",
    "            true_labels.extend(labels_batch)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    precision = precision_score(true_labels, preds)\n",
    "    recall = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    report = classification_report(true_labels, preds, digits=4)\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    \n",
    "    print(\"Validation Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Call the evaluate_on_validation function after training\n",
    "evaluate_on_validation(net, valid_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
